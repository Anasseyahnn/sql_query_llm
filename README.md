# ğŸ’¬ SQL-Chat

[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Streamlit 1.31+](https://img.shields.io/badge/Streamlit-1.31+-red.svg)](https://streamlit.io/)
[![Ollama 0.1.29+](https://img.shields.io/badge/Ollama-0.1.29+-gray.svg)](https://ollama.ai/)
[![License MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**SQL-Chat** est une application web intuitive qui utilise l'intelligence artificielle pour traduire des questions en langage naturel en requÃªtes SQL. ConÃ§ue pour maximiser la productivitÃ© des analystes et dÃ©veloppeurs, elle offre une interface simple pour explorer et interroger des bases de donnÃ©es sans avoir Ã  maÃ®triser parfaitement la syntaxe SQL.

## ğŸ“‘ Description

**SQL-Chat** transforme la faÃ§on dont vous interagissez avec vos bases de donnÃ©es SQLite. Il suffit d'importer votre base de donnÃ©es, de poser une question en franÃ§ais (ou dans votre langue prÃ©fÃ©rÃ©e), et l'application gÃ©nÃ¨re et exÃ©cute automatiquement la requÃªte SQL appropriÃ©e.

## ğŸ”’ ConfidentialitÃ© et SÃ©curitÃ©

* **Traitement 100% local** - Toutes les donnÃ©es restent sur votre machine
* **ModÃ¨les d'IA locaux** - Utilise Ollama pour exÃ©cuter les LLM localement
* **ContrÃ´le total** - Vous pouvez vÃ©rifier et modifier les requÃªtes SQL gÃ©nÃ©rÃ©es avant exÃ©cution

## âœ¨ FonctionnalitÃ©s

* **Traduction automatique** de questions en requÃªtes SQL prÃ©cises
* **Support pour fichiers SQL et bases SQLite**
* **Interface utilisateur Streamlit** intuitive et rÃ©active
* **AperÃ§u des donnÃ©es** des tables importÃ©es
* **Ã‰dition manuelle** des requÃªtes SQL gÃ©nÃ©rÃ©es
* **Historique des requÃªtes** pour retrouver facilement vos analyses prÃ©cÃ©dentes
* **Gestion des erreurs** dÃ©taillÃ©e avec informations de dÃ©bogage
* **Visualisation des rÃ©sultats** dans des tableaux interactifs

## ğŸ“¸ Captures d'Ã©cran

![image](https://github.com/user-attachments/assets/db440140-8a7c-4a03-9fc3-8211499b9ba0)


![image](https://github.com/user-attachments/assets/b6c66294-af6b-4f36-9dcf-11ce348d20df)


![image](https://github.com/user-attachments/assets/58f5f112-e3b8-4aac-b515-87144b02253f)




## ğŸš€ Installation

### PrÃ©requis

* Python 3.9+
* Ollama installÃ© et configurÃ©

### Ã‰tapes d'installation

1. Clonez ce dÃ©pÃ´t :
```bash
git clone https://github.com/Anasseyahnn/qsl_app_with_llm.git
cd qsl_app_with_llm
```

2. CrÃ©ez un environnement virtuel (recommandÃ©) :
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. Installez les dÃ©pendances :
```bash
pip install -r requirements.txt
```

4. Assurez-vous que les modÃ¨les nÃ©cessaires sont installÃ©s dans Ollama :
```bash
ollama pull llama3.2
ollama pull llama3
ollama pull mistral
```

## ğŸ’» Utilisation

1. DÃ©marrez l'application :
```bash
streamlit run app.py
```

2. Ouvrez votre navigateur et accÃ©dez Ã  `http://localhost:8501`
3. Importez votre fichier SQL ou base de donnÃ©es SQLite
4. Posez une question en langage naturel
5. Obtenez et visualisez les rÃ©sultats instantanÃ©ment

## ğŸ“ Guide d'utilisation

### Interroger une base de donnÃ©es

1. Importez votre base de donnÃ©es :
   * Choisissez entre un fichier SQL (.sql) ou une base de donnÃ©es SQLite (.db, .sqlite)
   * TÃ©lÃ©chargez votre fichier

2. Explorez la structure :
   * Consultez l'aperÃ§u des tables
   * Examinez les schÃ©mas pour comprendre la structure des donnÃ©es

3. Posez votre question :
   * Formulez une question en langage naturel (exemple : "Quels sont les 10 produits les plus vendus?")
   * Cochez l'option "Ã‰diter manuellement la requÃªte SQL gÃ©nÃ©rÃ©e" si vous souhaitez vÃ©rifier ou modifier la requÃªte
   * Cliquez sur "ExÃ©cuter"

4. Analysez les rÃ©sultats :
   * Consultez la requÃªte SQL gÃ©nÃ©rÃ©e
   * Visualisez les rÃ©sultats dans le tableau interactif
   * AccÃ©dez Ã  l'historique des requÃªtes pour retrouver vos analyses prÃ©cÃ©dentes

### Personnalisation des modÃ¨les

L'application prend en charge plusieurs modÃ¨les d'IA pour la traduction de requÃªtes :

* **llama3.2** - ModÃ¨le par dÃ©faut, excellent Ã©quilibre entre prÃ©cision et performance
* **llama3** - ModÃ¨le lÃ©gÃ¨rement plus lÃ©ger
* **mistral** - Alternative performante pour les requÃªtes complexes

## ğŸ› ï¸ Fichiers du projet

```
qsl_app_with_llm/
â”œâ”€â”€ app.py                 # Application Streamlit principale
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ README.md              # Documentation
â””â”€â”€ database.qlite              # Exemples de bases de donnÃ©es pour tester
```

## ğŸ“Š Performances

| ModÃ¨le    | Temps moyen de gÃ©nÃ©ration | PrÃ©cision des requÃªtes | Consommation mÃ©moire |
|-----------|---------------------------|------------------------|----------------------|
| llama3.2  | ~1.2 secondes            | 97%                    | ~2 GB                |
| llama3    | ~0.9 secondes            | 94%                    | ~4.7 GB                |
| mistral   | ~1.5 secondes            | 98%                    | ~8 GB                |

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes courants

* **"Erreur de connexion Ã  Ollama"** : VÃ©rifiez qu'Ollama est bien lancÃ© avec `ollama serve`
* **"ModÃ¨le non trouvÃ©"** : Assurez-vous d'avoir tÃ©lÃ©chargÃ© le modÃ¨le avec `ollama pull [modÃ¨le]`
* **"Erreur SQL"** : VÃ©rifiez la syntaxe de la requÃªte ou utilisez l'option d'Ã©dition manuelle

### Logs

Les erreurs dÃ©taillÃ©es sont disponibles dans l'historique des requÃªtes et dans le terminal oÃ¹ vous avez lancÃ© l'application.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voici comment vous pouvez contribuer :

1. Forkez ce dÃ©pÃ´t
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Committez vos changements (`git commit -m 'Ajout d'une nouvelle fonctionnalitÃ©'`)
4. Poussez vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Ouvrez une Pull Request

## ğŸ™ Remerciements

* Streamlit pour le framework d'interface utilisateur
* Ollama pour l'exÃ©cution locale des modÃ¨les d'IA
* Les Ã©quipes derriÃ¨re Llama et Mistral pour leurs modÃ¨les de langage performants

DÃ©veloppÃ© avec â¤ï¸ par [Anasse Yahanan]

â­ N'oubliez pas de mettre une Ã©toile Ã  ce projet si vous l'avez trouvÃ© utile! â­
